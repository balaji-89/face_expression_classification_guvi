{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt \n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img = np.array(self.images[idx])\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx]).type(torch.long)\n",
    "        sample = (img, label)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path='fer2013.csv'):\n",
    "    fer2013 = pd.read_csv(path)\n",
    "    emotion_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "\n",
    "    return fer2013, emotion_mapping\n",
    "\n",
    "def ensure_color(image):\n",
    "    if len(image.shape) == 2:\n",
    "        return np.dstack([image] * 3)\n",
    "    elif image.shape[2] == 1:\n",
    "        return np.dstack([image] * 3)\n",
    "    return image\n",
    "\n",
    "\n",
    "def ensure_gray(image):\n",
    "    try:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    except cv2.error:\n",
    "        pass\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    image_array = []\n",
    "    image_label = []\n",
    "\n",
    "    emotion_mapping = {'angry' : 0, 'disgust' : 1,'fear' : 2,'happy' : 3,'sad' : 4,'surprise' : 5, 'neutral' : 6}\n",
    "    test_root_path = '../dataset/test/'\n",
    "    for emotion_dir in os.listdir(test_root_path):\n",
    "        for img_name in os.listdir(os.path.join(test_root_path,emotion_dir)):\n",
    "            img = cv2.imread(os.path.join(test_root_path,emotion_dir,img_name))\n",
    "            assert isinstance(img, np.ndarray)\n",
    "            img = ensure_color(img)\n",
    "            img = cv2.resize(img, (224,224))\n",
    "            # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            image_array.append(img)\n",
    "            image_label.append(emotion_dir)\n",
    "    \n",
    "\n",
    "\n",
    "    image_label = list(map(lambda x: emotion_mapping[x], image_label))\n",
    "\n",
    "    return np.array(image_array), np.array(image_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(path='datasets/fer2013/fer2013.csv', bs=64, augment=True):\n",
    "    \"\"\" Prepare train, val, & test dataloaders\n",
    "        Augment training data using:\n",
    "            - cropping\n",
    "            - shifting (vertical/horizental)\n",
    "            - horizental flipping\n",
    "            - rotation\n",
    "\n",
    "        input: path to fer2013 csv file\n",
    "        output: (Dataloader, Dataloader, Dataloader) \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    xtest, ytest = prepare_data()\n",
    "    transform = transforms.Compose(\n",
    "    transforms=[ transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "    # X = np.vstack((xtrain, xval))\n",
    "    # Y = np.hstack((ytrain, yval))\n",
    "    test = CustomDataset(xtest, ytest,transform)\n",
    "    testloader = DataLoader(test, batch_size=2, shuffle=True)\n",
    "\n",
    "    return testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64, 3, 224, 224])\n",
      "tensor([6, 4, 6, 5, 3, 3, 4, 5, 4, 3, 3, 3, 2, 3, 6, 2, 5, 1, 3, 5, 5, 2, 4, 2,\n",
      "        3, 6, 0, 0, 6, 2, 3, 3, 2, 4, 5, 4, 6, 3, 5, 0, 3, 5, 4, 3, 5, 2, 6, 5,\n",
      "        6, 3, 3, 4, 0, 5, 2, 3, 6, 3, 4, 4, 2, 4, 5, 0])\n"
     ]
    }
   ],
   "source": [
    "for data in get_dataloaders():\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        print(labels.shape)\n",
    "        print(inputs.shape)\n",
    "        print(labels)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vgg(\n",
       "  (conv1a): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv1b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2a): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv2b): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3a): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv3b): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4a): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (conv4b): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bn1a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn1b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2a): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2b): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3a): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3b): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4a): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4b): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lin1): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "  (lin2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (lin3): Linear(in_features=4096, out_features=7, bias=True)\n",
       "  (drop): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Trained Model\n",
    "checkpoint = torch.load('./weights/VGGNet',map_location=torch.device('cpu'))\n",
    "net = Vgg().to(device)\n",
    "net.load_state_dict(checkpoint[\"params\"])\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Evalutae model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_count(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the top k corrrect count for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "\n",
    "def evaluate(net, dataloader, loss_fn, Ncrop, device):\n",
    "    net = net.eval()\n",
    "    loss_tr, n_samples = 0.0, 0.0\n",
    "\n",
    "    y_pred = []\n",
    "    y_gt = []\n",
    "\n",
    "    correct_count1 = 0\n",
    "    correct_count2 = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        if Ncrop:\n",
    "            # fuse crops and batchsize\n",
    "            bs, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # combine results across the crops\n",
    "            outputs = outputs.view(bs, ncrops, -1)\n",
    "            outputs = torch.sum(outputs, dim=1) / ncrops\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # calculate performance metrics\n",
    "        loss_tr += loss.item()\n",
    "\n",
    "        # accuracy\n",
    "        counts = correct_count(outputs, labels, topk=(1, 2))\n",
    "        correct_count1 += counts[0].item()\n",
    "        correct_count2 += counts[1].item()\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "\n",
    "        y_pred.extend(pred.item() for pred in preds)\n",
    "        y_gt.extend(y.item() for y in labels)\n",
    "\n",
    "    acc1 = 100 * correct_count1 / n_samples\n",
    "    acc2 = 100 * correct_count2 / n_samples\n",
    "    loss = loss_tr / n_samples\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Top 1 Accuracy: %2.6f %%\" % acc1)\n",
    "    print(\"Top 2 Accuracy: %2.6f %%\" % acc2)\n",
    "    print(\"Loss: %2.6f\" % loss)\n",
    "    print(\"Precision: %2.6f\" % precision_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"Recall: %2.6f\" % recall_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"F1 Score: %2.6f\" % f1_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_gt, y_pred), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rmn import RMN, get_emo_model\n",
    "net = get_emo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "--------------------------------------------------------\n",
      "Top 1 Accuracy: 50.626916 %\n",
      "Top 2 Accuracy: 70.130956 %\n",
      "Loss: 0.881979\n",
      "Precision: 0.506269\n",
      "Recall: 0.506269\n",
      "F1 Score: 0.506269\n",
      "Confusion Matrix:\n",
      " [[ 614   53   35   16   93   76   71]\n",
      " [  55   38    0    3   11    1    3]\n",
      " [ 322   17  130   37  241  172  105]\n",
      " [ 114   21   31 1255   50  248   55]\n",
      " [ 353   27   38   47  503   93  186]\n",
      " [ 105    4  123   23   15  545   16]\n",
      " [ 244   12   24   96  165  143  549]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Get data with no augmentation\n",
    "testloader = get_dataloaders(augment=False)\n",
    "print(\"Test\")\n",
    "evaluate(net, testloader, criterion, False, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_exp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
