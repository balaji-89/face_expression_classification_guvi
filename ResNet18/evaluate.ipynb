{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "from dataset import get_dataloaders\n",
    "from utils import get_model\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='USTC Computer Vision Final Project')\n",
    "parser.add_argument('--batch_size', default=128, type=int)\n",
    "parser.add_argument('--seed', default=0, type=int)\n",
    "parser.add_argument('--data_path', default='datasets/fer2013/fer2013.csv', type=str)\n",
    "parser.add_argument('--checkpoint', default='', type=str)\n",
    "parser.add_argument('--arch', default=\"ResNet18\", type=str)\n",
    "parser.add_argument('--Ncrop', default=True, type=eval)\n",
    "\n",
    "\n",
    "def correct_count(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the top k corrrect count for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].contiguous().view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k)\n",
    "    return res\n",
    "\n",
    "\n",
    "def evaluate(net, dataloader, loss_fn, Ncrop, device):\n",
    "    net = net.eval()\n",
    "    loss_tr, n_samples = 0.0, 0.0\n",
    "\n",
    "    y_pred = []\n",
    "    y_gt = []\n",
    "\n",
    "    correct_count1 = 0\n",
    "    correct_count2 = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        if Ncrop:\n",
    "            # fuse crops and batchsize\n",
    "            bs, ncrops, c, h, w = inputs.shape\n",
    "            inputs = inputs.view(-1, c, h, w)\n",
    "\n",
    "            # forward\n",
    "            outputs = net(inputs)\n",
    "\n",
    "            # combine results across the crops\n",
    "            outputs = outputs.view(bs, ncrops, -1)\n",
    "            outputs = torch.sum(outputs, dim=1) / ncrops\n",
    "        else:\n",
    "            outputs = net(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # calculate performance metrics\n",
    "        loss_tr += loss.item()\n",
    "\n",
    "        # accuracy\n",
    "        counts = correct_count(outputs, labels, topk=(1, 2))\n",
    "        correct_count1 += counts[0].item()\n",
    "        correct_count2 += counts[1].item()\n",
    "\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        n_samples += labels.size(0)\n",
    "\n",
    "        y_pred.extend(pred.item() for pred in preds)\n",
    "        y_gt.extend(y.item() for y in labels)\n",
    "\n",
    "    acc1 = 100 * correct_count1 / n_samples\n",
    "    acc2 = 100 * correct_count2 / n_samples\n",
    "    loss = loss_tr / n_samples\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Top 1 Accuracy: %2.6f %%\" % acc1)\n",
    "    print(\"Top 2 Accuracy: %2.6f %%\" % acc2)\n",
    "    print(\"Loss: %2.6f\" % loss)\n",
    "    print(\"Precision: %2.6f\" % precision_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"Recall: %2.6f\" % recall_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"F1 Score: %2.6f\" % f1_score(y_gt, y_pred, average='micro'))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_gt, y_pred), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = get_model().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/laptop-kl-11/personal_project/face_expression_classification/dataset/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m         evaluate(model, test_loader, loss_fn, \u001b[38;5;28;01mTrue\u001b[39;00m, device)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./weights/best_checkpoint.tar\u001b[39m\u001b[38;5;124m'\u001b[39m,map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 8\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/personal_project/face_expression_classification_guvi/ResNet18/dataset.py:81\u001b[0m, in \u001b[0;36mget_dataloaders\u001b[0;34m(path, bs, augment)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataloaders\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatasets/fer2013/fer2013.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, bs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Prepare train, val, & test dataloaders\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m        Augment training data using:\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m            - cropping\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;124;03m        input: path to fer2013 csv file\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m        output: (Dataloader, Dataloader, Dataloader) \"\"\"\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m     xtest, ytest \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m     mu, st \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     83\u001b[0m     test_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     84\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mGrayscale(),\n\u001b[1;32m     85\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mTenCrop(\u001b[38;5;241m40\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     89\u001b[0m             [transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m(mu,), std\u001b[38;5;241m=\u001b[39m(st,))(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tensors])),\n\u001b[1;32m     90\u001b[0m     ])\n",
      "File \u001b[0;32m~/personal_project/face_expression_classification_guvi/ResNet18/dataset.py:55\u001b[0m, in \u001b[0;36mprepare_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m emotion_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mangry\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisgust\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfear\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m2\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhappy\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m3\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msad\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m4\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurprise\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m6\u001b[39m}\n\u001b[1;32m     54\u001b[0m test_root_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/laptop-kl-11/personal_project/face_expression_classification/dataset/test\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m emotion_dir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_root_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_root_path,emotion_dir)):\n\u001b[1;32m     57\u001b[0m         img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(test_root_path,emotion_dir,img_name),cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/laptop-kl-11/personal_project/face_expression_classification/dataset/test'"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # args = parser.parse_args()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    checkpoint = torch.load('./weights/best_checkpoint.tar',map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    test_loader = get_dataloaders(augment=False)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        print(\"Test\")\n",
    "        evaluate(model, test_loader, loss_fn, True, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "--------------------------------------------------------\n",
      "Top 1 Accuracy: 73.711340 %\n",
      "Top 2 Accuracy: 86.263583 %\n",
      "Loss: 0.014132\n",
      "Precision: 0.737113\n",
      "Recall: 0.737113\n",
      "F1 Score: 0.737113\n",
      "Confusion Matrix:\n",
      " [[ 643    5   76   25  127   16   66]\n",
      " [  24   73    3    3    5    1    2]\n",
      " [  91    2  576   23  185   67   80]\n",
      " [  27    0   15 1584   27   38   83]\n",
      " [  99    0  102   32  805   13  196]\n",
      " [  15    0   54   32   15  701   14]\n",
      " [  50    1   41   57  159   16  909]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # args = parser.parse_args()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    checkpoint = torch.load('/home/laptop-kl-11/personal_project/face_expression_classification/ResNet18/weights/best_checkpoint.tar',map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    test_loader = get_dataloaders(augment=False)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        print(\"Test\")\n",
    "        evaluate(model, test_loader, loss_fn, True, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
